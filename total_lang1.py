import streamlit as st
import os
import json
import numpy as np
import urllib.request
import urllib.parse
import re
import pandas as pd
from datetime import datetime
import asyncio
import aiohttp
from typing import TypedDict, List, Dict, Optional, Literal, Annotated
from bs4 import BeautifulSoup
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import operator

from supabase import create_client
from openai import OpenAI
from sentence_transformers import SentenceTransformer

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# LangChain imports for tracing
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LangSmith ì¶”ì  ì„¤ì •
try:
    from langsmith import Client
    from langchain_core.tracers.context import tracing_v2_enabled
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    
# í˜ì´ì§€ êµ¬ì„±
st.set_page_config(page_title="ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ íŒŒì¸ë” (LangGraph Enhanced)", layout="wide")

# ========== API í‚¤ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ==========
def init_clients():
    """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    try:
        # Streamlit Cloud í™˜ê²½ì—ì„œëŠ” st.secrets ì‚¬ìš©
        if hasattr(st, 'secrets') and st.secrets:
            supabase_url = st.secrets["SUPABASE_URL"]
            supabase_key = st.secrets["SUPABASE_KEY"]
            openai_api_key = st.secrets["OPENAI_API_KEY"]
            NAVER_CLIENT_ID = st.secrets["NAVER_CLIENT_ID"]
            NAVER_CLIENT_SECRET = st.secrets["NAVER_CLIENT_SECRET"]
            LANGCHAIN_API_KEY = st.secrets.get("LANGCHAIN_API_KEY")
            LANGCHAIN_PROJECT = st.secrets.get("LANGCHAIN_PROJECT", "smart-shopping-finder")
        else:
            # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
            supabase_url = os.environ.get("SUPABASE_URL")
            supabase_key = os.environ.get("SUPABASE_KEY")
            openai_api_key = os.environ.get("OPENAI_API_KEY")
            NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
            NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
            LANGCHAIN_API_KEY = os.environ.get("LANGCHAIN_API_KEY")
            LANGCHAIN_PROJECT = os.environ.get("LANGCHAIN_PROJECT", "smart-shopping-finder")

        # API í‚¤ í™•ì¸
        if not all([supabase_url, supabase_key, openai_api_key, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET]):
            st.error("í•„ìš”í•œ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.stop()

        # LangSmith ì„¤ì •
        if LANGCHAIN_API_KEY:
            os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
            os.environ["LANGCHAIN_PROJECT"] = LANGCHAIN_PROJECT
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
            st.sidebar.success(f"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨: {LANGCHAIN_PROJECT}")
        else:
            st.sidebar.warning("âš ï¸ LangSmith API í‚¤ê°€ ì—†ì–´ ì¶”ì ì´ ë¹„í™œì„±í™”ë¨")
            os.environ["LANGCHAIN_TRACING_V2"] = "false"

        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase = create_client(supabase_url, supabase_key)
        openai_client = OpenAI(api_key=openai_api_key)
        
        # LangChain ChatOpenAI ì´ˆê¸°í™” (ì¶”ì ì„ ìœ„í•´)
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            api_key=openai_api_key
        )
        
        return supabase, openai_client, llm, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET
        
    except Exception as e:
        st.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase, openai_client, llm, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET = init_clients()

# ========== ìƒíƒœ ì •ì˜ ==========
class SearchState(TypedDict):
    query: str
    source_type: str
    search_mode: str
    naver_results: List[Dict]
    full_articles: List[Dict]
    embeddings: List[List[float]]
    semantic_results: List[Dict]
    final_answer: str
    error: Optional[str]
    retry_count: int
    quality_score: float
    status_messages: Annotated[List[str], operator.add]

# ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ==========
@st.cache_resource
def load_embedding_model():
    """í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
    try:
        model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        return model
    except:
        try:
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            return model
        except Exception as e:
            st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return None

embedding_model = load_embedding_model()

def generate_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    if not embedding_model:
        return None
        
    try:
        if not text or len(text.strip()) < 10:
            return None
        
        cleaned_text = re.sub(r'\s+', ' ', text.strip())
        cleaned_text = re.sub(r'[^\w\sê°€-í£\.]', ' ', cleaned_text)
        
        if len(cleaned_text) > 512:
            cleaned_text = cleaned_text[:512]
        
        embedding = embedding_model.encode(cleaned_text, convert_to_tensor=False)
        embedding_list = embedding.tolist() if hasattr(embedding, 'tolist') else embedding
        
        # 768ì°¨ì›ì„ 1536ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
        if len(embedding_list) == 768:
            return embedding_list + [0.0] * (1536 - 768)
        elif len(embedding_list) == 1536:
            return embedding_list
        else:
            if len(embedding_list) < 1536:
                return embedding_list + [0.0] * (1536 - len(embedding_list))
            else:
                return embedding_list[:1536]
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None

# ========== LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ==========
def search_naver_node(state: SearchState) -> SearchState:
    """ë„¤ì´ë²„ API ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        # API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
        api_endpoint = {
            "ë¸”ë¡œê·¸": "blog",
            "ë‰´ìŠ¤": "news", 
            "ì‡¼í•‘": "shop"
        }.get(state["source_type"], "blog")
        
        encoded_query = urllib.parse.quote(state["query"])
        url = f"https://openapi.naver.com/v1/search/{api_endpoint}?query={encoded_query}&display=20&sort=sim"
        
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
        request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
        
        with urllib.request.urlopen(request, timeout=15) as response:
            if response.getcode() == 200:
                response_data = json.loads(response.read().decode('utf-8'))
                items = response_data.get('items', [])
                state["naver_results"] = items
                state["status_messages"] = [f"âœ… {len(items)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."]
            else:
                state["error"] = "ë„¤ì´ë²„ API ì˜¤ë¥˜"
                
    except Exception as e:
        state["error"] = str(e)
        
    return state

def fetch_full_articles_node(state: SearchState) -> SearchState:
    """ì „ì²´ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ë…¸ë“œ"""
    if state["source_type"] != "ë‰´ìŠ¤" or state["search_mode"] != "ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥":
        state["full_articles"] = state["naver_results"]
        return state
    
    def fetch_article(item):
        url = item.get('link', '')
        if url:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                request = urllib.request.Request(url, headers=headers)
                with urllib.request.urlopen(request, timeout=10) as response:
                    html = response.read().decode('utf-8')
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
                    article_body = soup.find('article', {'id': 'dic_area'})
                    if article_body:
                        item_copy = item.copy()
                        item_copy['full_content'] = article_body.get_text(strip=True)
                        return item_copy
            except:
                pass
        return item
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=5) as executor:
        full_articles = list(executor.map(fetch_article, state["naver_results"][:10]))
    
    # ë‚˜ë¨¸ì§€ í•­ëª©ë“¤ë„ ì¶”ê°€
    full_articles.extend(state["naver_results"][10:])
    
    state["full_articles"] = full_articles
    full_count = sum(1 for a in full_articles if 'full_content' in a)
    if full_count > 0:
        state["status_messages"] = [f"âœ… {full_count}ê°œì˜ ì „ì²´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤."]
    
    return state

def save_to_database_node(state: SearchState) -> SearchState:
    """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë…¸ë“œ"""
    if state["search_mode"] != "ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥":
        return state
        
    saved_count = 0
    articles = state.get("full_articles", state["naver_results"])
    
    for item in articles:
        try:
            # HTML íƒœê·¸ ì œê±°
            title = re.sub('<[^<]+?>', '', item.get('title', ''))
            
            # ì „ì²´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ description ì‚¬ìš©
            if 'full_content' in item and item['full_content']:
                content = item['full_content']
            else:
                content = re.sub('<[^<]+?>', '', item.get('description', ''))
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            if state["source_type"] == "ë‰´ìŠ¤":
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'publisher': item.get('originallink', '').replace('https://', '').replace('http://', '').split('/')[0] if item.get('originallink') else '',
                    'date': item.get('pubDate', ''),
                    'collection': state["source_type"]
                }
                full_text = f"ë‰´ìŠ¤ ì œëª©: {title}\në‰´ìŠ¤ ë‚´ìš©: {content}\nì–¸ë¡ ì‚¬: {metadata.get('publisher', '')}"
            elif state["source_type"] == "ë¸”ë¡œê·¸":
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'bloggername': item.get('bloggername', ''),
                    'date': item.get('postdate', ''),
                    'collection': state["source_type"]
                }
                full_text = f"ì œëª©: {title}\në‚´ìš©: {content}\në¸”ë¡œê±°: {metadata.get('bloggername', '')}"
            else:  # ì‡¼í•‘
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'lprice': item.get('lprice', ''),
                    'mallname': item.get('mallName', ''),
                    'brand': item.get('brand', ''),
                    'collection': state["source_type"]
                }
                full_text = f"ìƒí’ˆëª…: {title}\nì„¤ëª…: {content}\në¸Œëœë“œ: {metadata.get('brand', '')}"
            
            # ì„ë² ë”© ìƒì„±
            embedding = generate_embedding(full_text)
            if embedding:
                # ì¤‘ë³µ ì²´í¬
                existing = supabase.table('documents').select('id').eq('metadata->>url', metadata.get('url', '')).execute()
                
                if not existing.data:
                    data = {
                        'content': full_text,
                        'embedding': embedding,
                        'metadata': metadata
                    }
                    supabase.table('documents').insert(data).execute()
                    saved_count += 1
                    
        except Exception as e:
            continue
    
    if saved_count > 0:
        state["status_messages"] = [f"âœ… {saved_count}ê°œì˜ ë¬¸ì„œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤."]
    
    return state

def semantic_search_node(state: SearchState) -> SearchState:
    """ì‹œë§¨í‹± ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        if state["source_type"] == "ë‰´ìŠ¤":
            processed_query = f"ë‰´ìŠ¤ ê²€ìƒ‰: {state['query']} ë‰´ìŠ¤ ê¸°ì‚¬ ì–¸ë¡ ì‚¬ ë³´ë„"
        elif state["source_type"] == "ì‡¼í•‘":
            processed_query = f"ìƒí’ˆ ê²€ìƒ‰: {state['query']} ì‡¼í•‘ ìƒí’ˆ ê°€ê²©"
        else:
            processed_query = f"ë¸”ë¡œê·¸ ê²€ìƒ‰: {state['query']} ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…"
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = generate_embedding(processed_query)
        
        if query_embedding is None:
            state["semantic_results"] = []
            return state
        
        # ë²¡í„° ê²€ìƒ‰
        response = supabase.rpc(
            'match_documents',
            {
                'query_embedding': query_embedding,
                'match_threshold': 0.3,
                'match_count': 50
            }
        ).execute()
        
        # ì†ŒìŠ¤ íƒ€ì… í•„í„°ë§
        filtered_results = []
        if response.data:
            for item in response.data:
                metadata = item.get('metadata', {})
                if isinstance(metadata, str):
                    metadata = json.loads(metadata)
                
                if metadata.get('collection') == state["source_type"]:
                    filtered_results.append(item)
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        filtered_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        state["semantic_results"] = filtered_results[:10]
        state["status_messages"] = [f"âœ… {len(state['semantic_results'])}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."]
        
    except Exception as e:
        state["error"] = f"ì‹œë§¨í‹± ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        state["semantic_results"] = []
        
    return state

def generate_answer_node(state: SearchState) -> SearchState:
    """AI ë‹µë³€ ìƒì„± ë…¸ë“œ (LangChain ì‚¬ìš©)"""
    if not state["semantic_results"]:
        state["final_answer"] = f"'{state['query']}'ì— ëŒ€í•œ {state['source_type']} ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    contexts = []
    for i, result in enumerate(state["semantic_results"][:5]):
        content = result['content']
        metadata = result.get('metadata', {})
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        
        title = metadata.get('title', 'ì œëª© ì—†ìŒ')
        similarity = result.get('similarity', 0) * 100
        
        contexts.append(f"ë¬¸ì„œ {i+1} - {title} (ìœ ì‚¬ë„: {similarity:.1f}%):\n{content}\n")
    
    context_text = "\n".join(contexts)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompts = {
        "ë¸”ë¡œê·¸": """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¸”ë¡œê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , êµ¬ì²´ì ì¸ íŒì´ë‚˜ ê²½í—˜ë‹´ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.""",
        
        "ë‰´ìŠ¤": """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ì‹¤ì ì´ê³  ê· í˜•ì¡íŒ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ìµœì‹  ë™í–¥ê³¼ ë‹¤ì–‘í•œ ê´€ì ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.""",
        
        "ì‡¼í•‘": """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ì‡¼í•‘ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ìƒí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì‡¼í•‘ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ê°€ê²© ë¹„êµë‚˜ ì œí’ˆ íŠ¹ì§•ì„ í¬í•¨í•´ ì£¼ì„¸ìš”."""
    }
    
    # LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompts[state["source_type"]]),
        ("human", """ë‹¤ìŒì€ ë„¤ì´ë²„ {source_type}ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {query}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ìœ ìš©í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.""")
    ])
    
    try:
        # LangChain ì²´ì¸ ì‹¤í–‰ (ì´ë ‡ê²Œ í•˜ë©´ ìë™ìœ¼ë¡œ ì¶”ì ë¨)
        chain = prompt | llm | StrOutputParser()
        
        answer = chain.invoke({
            "source_type": state["source_type"],
            "context": context_text,
            "query": state["query"]
        })
        
        state["final_answer"] = answer
        state["status_messages"] = ["âœ… AI ë‹µë³€ ìƒì„± ì™„ë£Œ!"]
        
    except Exception as e:
        state["error"] = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        state["final_answer"] = "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
    return state

def check_error(state: SearchState) -> str:
    """ì—ëŸ¬ ì²´í¬ ë…¸ë“œ"""
    if state.get("error"):
        return "error"
    return "continue"

# ========== LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ==========
def create_search_workflow():
    """ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("search_naver", search_naver_node)
    workflow.add_node("fetch_articles", fetch_full_articles_node)
    workflow.add_node("save_database", save_to_database_node)
    workflow.add_node("semantic_search", semantic_search_node)
    workflow.add_node("generate_answer", generate_answer_node)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("search_naver")
    
    # ì¡°ê±´ë¶€ ì—£ì§€
    workflow.add_conditional_edges(
        "search_naver",
        check_error,
        {
            "continue": "fetch_articles",
            "error": END
        }
    )
    
    workflow.add_edge("fetch_articles", "save_database")
    workflow.add_edge("save_database", "semantic_search")
    workflow.add_edge("semantic_search", "generate_answer")
    workflow.add_edge("generate_answer", END)
    
    # ì²´í¬í¬ì¸í„° ì¶”ê°€
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    
    return app

# ì›Œí¬í”Œë¡œìš° ìƒì„±
search_app = create_search_workflow()

# ========== Streamlit UI ==========
st.title("ğŸ›ï¸ ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ íŒŒì¸ë”: LangGraph Enhanced")
st.write("AI ì—ì´ì „íŠ¸ê°€ ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ”§ ì„¤ì •")
    
    search_mode = st.radio(
        "ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ",
        options=["ì‹œë§¨í‹± ê²€ìƒ‰ (ì €ì¥ëœ ë°ì´í„°)", "ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"],
        index=0,
        help="ì €ì¥ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
    )
    
    st.markdown("---")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
    st.markdown("### ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ")
    try:
        result = supabase.table('documents').select('id', count='exact').execute()
        doc_count = result.count if hasattr(result, 'count') else len(result.data)
        st.metric("ì´ ë¬¸ì„œ ìˆ˜", f"{doc_count:,}ê°œ")
    except Exception as e:
        st.error("DB ì—°ê²° ì˜¤ë¥˜")

# ë©”ì¸ ì½˜í…ì¸ 
col1, col2 = st.columns([3, 1])

with col1:
    # ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ
    source_options = ["ì‡¼í•‘", "ë¸”ë¡œê·¸", "ë‰´ìŠ¤"]
    selected_source = st.radio(
        "ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ",
        options=source_options,
        horizontal=True
    )
    
    # ì§ˆë¬¸ ì…ë ¥
    query = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
        placeholder=f"{selected_source} ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì•„ì´í° 15 ì¥ì ê³¼ ë‹¨ì )",
        help=f"{selected_source}ì—ì„œ ê²€ìƒ‰í•  ë‚´ìš©ì„ ìì„¸íˆ ì…ë ¥í•´ì£¼ì„¸ìš”."
    )

with col2:
    st.markdown("### ğŸ¯ ê²€ìƒ‰ íŒ")
    st.markdown("""
    - êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš©
    - ë¸Œëœë“œëª…ì´ë‚˜ ëª¨ë¸ëª… í¬í•¨
    - ê°€ê²©ëŒ€ë‚˜ ì¡°ê±´ ëª…ì‹œ
    """)

# ê²€ìƒ‰ ì‹¤í–‰
if st.button(f"ğŸ” {selected_source}ì—ì„œ ê²€ìƒ‰í•˜ê¸°", type="primary", use_container_width=True):
    if query.strip():
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {
                "query": query,
                "source_type": selected_source,
                "search_mode": search_mode,
                "naver_results": [],
                "full_articles": [],
                "embeddings": [],
                "semantic_results": [],
                "final_answer": "",
                "error": None,
                "retry_count": 0,
                "quality_score": 0.0,
                "status_messages": []
            }
            
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            config = {"configurable": {"thread_id": f"{query}_{selected_source}_{datetime.now().isoformat()}"}}
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ê° ë‹¨ê³„ë³„ë¡œ ì—…ë°ì´íŠ¸ ë°›ê¸°
            step_count = 0
            total_steps = 5
            
            for event in search_app.stream(initial_state, config):
                step_count += 1
                progress = int((step_count / total_steps) * 100)
                progress_bar.progress(progress)
                
                # ê° ë…¸ë“œì˜ ê²°ê³¼ì—ì„œ ìƒíƒœ ë©”ì‹œì§€ ì¶”ì¶œ
                for node_name, node_state in event.items():
                    if node_state.get("status_messages"):
                        for msg in node_state["status_messages"]:
                            status_text.info(msg)
                            time.sleep(0.5)  # ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ë³¼ ìˆ˜ ìˆë„ë¡
                    
                    # ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸
                    if "final_answer" in node_state:
                        final_state = node_state
            
            # ì—ëŸ¬ ì²´í¬
            if final_state.get("error"):
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {final_state['error']}")
                st.stop()
            
            # ê²°ê³¼ í‘œì‹œ
            st.markdown("---")
            st.markdown("## ğŸ¤– AI ë‹µë³€")
            st.markdown(final_state.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
            
            # ìƒì„¸ ê²°ê³¼ í‘œì‹œ
            if final_state.get("semantic_results"):
                with st.expander("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë³´ê¸°", expanded=False):
                    st.markdown(f"### {len(final_state['semantic_results'])}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤")
                    
                    for i, item in enumerate(final_state["semantic_results"]):
                        metadata = item.get('metadata', {})
                        if isinstance(metadata, str):
                            metadata = json.loads(metadata)
                        
                        similarity = item.get('similarity', 0) * 100
                        title = metadata.get('title', 'ì œëª© ì—†ìŒ')
                        
                        with st.container():
                            st.markdown(f"**{i+1}. {title}** (ìœ ì‚¬ë„: {similarity:.1f}%)")
                            
                            # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                            col1, col2 = st.columns(2)
                            with col1:
                                if metadata.get('url'):
                                    st.markdown(f"ğŸ”— [ì›ë³¸ ë³´ê¸°]({metadata['url']})")
                            with col2:
                                if selected_source == "ë‰´ìŠ¤" and metadata.get('publisher'):
                                    st.markdown(f"ğŸ“° {metadata['publisher']}")
                                elif selected_source == "ë¸”ë¡œê·¸" and metadata.get('bloggername'):
                                    st.markdown(f"âœï¸ {metadata['bloggername']}")
                                elif selected_source == "ì‡¼í•‘" and metadata.get('brand'):
                                    st.markdown(f"ğŸ·ï¸ {metadata['brand']}")
                            
                            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                            content_preview = item['content'][:200] + "..." if len(item['content']) > 200 else item['content']
                            st.markdown(f"> {content_preview}")
                            st.markdown("---")
            
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        finally:
            # ì§„í–‰ ìƒí™© í‘œì‹œ ì œê±°
            progress_bar.empty()
            status_text.empty()
    else:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
with st.expander("ğŸš€ LangGraph ê°œì„ ì‚¬í•­", expanded=False):
    st.markdown("""
    **ìƒˆë¡œìš´ ê¸°ëŠ¥:**
    - ğŸ”„ **ì§€ëŠ¥ì  ì›Œí¬í”Œë¡œìš°**: LangGraphë¡œ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤
    - ğŸ“Š **LangSmith ì¶”ì **: ëª¨ë“  ë‹¨ê³„ê°€ ìë™ìœ¼ë¡œ ì¶”ì ë˜ì–´ ë””ë²„ê¹… ìš©ì´
    - âš¡ **ë³‘ë ¬ ì²˜ë¦¬**: ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ë™ì‹œì— ê°€ì ¸ì™€ ì†ë„ í–¥ìƒ
    - ğŸ§  **ìƒíƒœ ê´€ë¦¬**: ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ì¶”ì 
    - ğŸ” **ì—ëŸ¬ í•¸ë“¤ë§**: ê° ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
    - ğŸ’¾ **ìºì‹± ìµœì í™”**: ì„ë² ë”© ëª¨ë¸ê³¼ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    
    **LangSmithì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ì •ë³´:**
    - ğŸ¯ ê° ë…¸ë“œì˜ ì‹¤í–‰ ì‹œê°„ê³¼ ì„±ëŠ¥
    - ğŸ“ ì…ë ¥/ì¶œë ¥ ë°ì´í„° ì¶”ì 
    - ğŸ” í”„ë¡¬í”„íŠ¸ì™€ LLM ì‘ë‹µ ë‚´ìš©
    - âš ï¸ ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸
    - ğŸ“ˆ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”
    
    **ì‚¬ìš© ë°©ë²•:**
    1. LangSmithì—ì„œ í”„ë¡œì íŠ¸ëª… 'smart-shopping-finder' í™•ì¸
    2. ê° ê²€ìƒ‰ ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ thread_idë¡œ ì¶”ì 
    3. ì›Œí¬í”Œë¡œìš°ì˜ ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼ í™•ì¸ ê°€ëŠ¥
    """)
