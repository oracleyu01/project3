import streamlit as st
import os
import json
import numpy as np
import urllib.request
import urllib.parse
import re
import pandas as pd
from datetime import datetime
import asyncio
import aiohttp
from typing import TypedDict, List, Dict, Optional, Literal
from bs4 import BeautifulSoup
import time
import threading
from concurrent.futures import ThreadPoolExecutor

from supabase import create_client
from openai import OpenAI
from sentence_transformers import SentenceTransformer

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# LangSmith ì¶”ì  ì„¤ì •
try:
    from langsmith import Client
    from langchain_core.tracers.context import tracing_v2_enabled
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    
# í˜ì´ì§€ êµ¬ì„±
st.set_page_config(page_title="ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ íŒŒì¸ë” (LangGraph Enhanced)", layout="wide")

# ========== API í‚¤ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ==========
def init_clients():
    """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    try:
        # Streamlit Cloud í™˜ê²½ì—ì„œëŠ” st.secrets ì‚¬ìš©
        if hasattr(st, 'secrets') and st.secrets:
            supabase_url = st.secrets["SUPABASE_URL"]
            supabase_key = st.secrets["SUPABASE_KEY"]
            openai_api_key = st.secrets["OPENAI_API_KEY"]
            NAVER_CLIENT_ID = st.secrets["NAVER_CLIENT_ID"]
            NAVER_CLIENT_SECRET = st.secrets["NAVER_CLIENT_SECRET"]
            LANGCHAIN_API_KEY = st.secrets.get("LANGCHAIN_API_KEY")
            LANGCHAIN_PROJECT = st.secrets.get("LANGCHAIN_PROJECT", "smart-shopping-finder")
        else:
            # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
            supabase_url = os.environ.get("SUPABASE_URL")
            supabase_key = os.environ.get("SUPABASE_KEY")
            openai_api_key = os.environ.get("OPENAI_API_KEY")
            NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
            NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
            LANGCHAIN_API_KEY = os.environ.get("LANGCHAIN_API_KEY")
            LANGCHAIN_PROJECT = os.environ.get("LANGCHAIN_PROJECT", "smart-shopping-finder")

        # API í‚¤ í™•ì¸
        if not all([supabase_url, supabase_key, openai_api_key, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET]):
            st.error("í•„ìš”í•œ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.stop()

        # LangSmith ì„¤ì •
        if LANGCHAIN_API_KEY:
            os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
            os.environ["LANGCHAIN_PROJECT"] = LANGCHAIN_PROJECT
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
            st.sidebar.success(f"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨")
        else:
            st.sidebar.warning("âš ï¸ LangSmith API í‚¤ê°€ ì—†ì–´ ì¶”ì ì´ ë¹„í™œì„±í™”ë¨")
            os.environ["LANGCHAIN_TRACING_V2"] = "false"

        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase = create_client(supabase_url, supabase_key)
        openai_client = OpenAI(api_key=openai_api_key)
        
        return supabase, openai_client, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET
        
    except Exception as e:
        st.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase, openai_client, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET = init_clients()

# ========== ìƒíƒœ ì •ì˜ ==========
class SearchState(TypedDict):
    query: str
    source_type: str
    search_mode: str
    naver_results: List[Dict]
    full_articles: List[Dict]
    embeddings: List[List[float]]
    semantic_results: List[Dict]
    final_answer: str
    error: Optional[str]
    retry_count: int
    quality_score: float
    status_message: str

# ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ==========
@st.cache_resource
def load_embedding_model():
    """í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
    try:
        model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        return model
    except:
        try:
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            return model
        except Exception as e:
            st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return None

embedding_model = load_embedding_model()

def generate_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    if not embedding_model:
        return None
        
    try:
        if not text or len(text.strip()) < 10:
            return None
        
        cleaned_text = re.sub(r'\s+', ' ', text.strip())
        cleaned_text = re.sub(r'[^\w\sê°€-í£\.]', ' ', cleaned_text)
        
        if len(cleaned_text) > 512:
            cleaned_text = cleaned_text[:512]
        
        embedding = embedding_model.encode(cleaned_text, convert_to_tensor=False)
        embedding_list = embedding.tolist() if hasattr(embedding, 'tolist') else embedding
        
        # 768ì°¨ì›ì„ 1536ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
        if len(embedding_list) == 768:
            return embedding_list + [0.0] * (1536 - 768)
        elif len(embedding_list) == 1536:
            return embedding_list
        else:
            if len(embedding_list) < 1536:
                return embedding_list + [0.0] * (1536 - len(embedding_list))
            else:
                return embedding_list[:1536]
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None

# ========== ë™ê¸° ë²„ì „ì˜ í•¨ìˆ˜ë“¤ ==========
def fetch_full_article_sync(url: str) -> Optional[str]:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ì „ì²´ ë‚´ìš© ë™ê¸°ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        request = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(request, timeout=10) as response:
            html = response.read().decode('utf-8')
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('article', {'id': 'dic_area'})
            if article_body:
                return article_body.get_text(strip=True)
            
            # ë‹¤ë¥¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì„ íƒìë“¤
            selectors = [
                'div.article_body', 'div.news_view', 'div.articleBody',
                'div.content', 'div.article-body', 'div#articleBodyContents'
            ]
            for selector in selectors:
                content = soup.select_one(selector)
                if content:
                    return content.get_text(strip=True)
            
            return None
    except Exception as e:
        return None

def search_naver_sync(query: str, source_type: str) -> Dict:
    """ë„¤ì´ë²„ API ê²€ìƒ‰ (ë™ê¸°)"""
    try:
        # API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
        api_endpoint = {
            "ë¸”ë¡œê·¸": "blog",
            "ë‰´ìŠ¤": "news", 
            "ì‡¼í•‘": "shop"
        }.get(source_type, "blog")
        
        encoded_query = urllib.parse.quote(query)
        url = f"https://openapi.naver.com/v1/search/{api_endpoint}?query={encoded_query}&display=20&sort=sim"
        
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
        request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
        
        with urllib.request.urlopen(request, timeout=15) as response:
            if response.getcode() == 200:
                response_data = json.loads(response.read().decode('utf-8'))
                items = response_data.get('items', [])
                return {"success": True, "items": items}
            else:
                return {"success": False, "error": "ë„¤ì´ë²„ API ì˜¤ë¥˜"}
                
    except Exception as e:
        return {"success": False, "error": str(e)}

def fetch_full_articles_sync(results: List[Dict], source_type: str) -> List[Dict]:
    """ì „ì²´ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° (ë³‘ë ¬ ì²˜ë¦¬)"""
    if source_type != "ë‰´ìŠ¤":
        return results
    
    def fetch_article(item):
        url = item.get('link', '')
        if url:
            full_content = fetch_full_article_sync(url)
            if full_content:
                item_copy = item.copy()
                item_copy['full_content'] = full_content
                return item_copy
        return item
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=5) as executor:
        full_articles = list(executor.map(fetch_article, results[:10]))
    
    # ë‚˜ë¨¸ì§€ í•­ëª©ë“¤ë„ ì¶”ê°€
    full_articles.extend(results[10:])
    
    return full_articles

def save_to_supabase_sync(articles: List[Dict], source_type: str) -> int:
    """Supabaseì— ì €ì¥ (ë™ê¸°)"""
    saved_count = 0
    
    for item in articles:
        try:
            # HTML íƒœê·¸ ì œê±°
            title = re.sub('<[^<]+?>', '', item.get('title', ''))
            
            # ì „ì²´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ description ì‚¬ìš©
            if 'full_content' in item and item['full_content']:
                content = item['full_content']
            else:
                content = re.sub('<[^<]+?>', '', item.get('description', ''))
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            if source_type == "ë‰´ìŠ¤":
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'publisher': item.get('originallink', '').replace('https://', '').replace('http://', '').split('/')[0] if item.get('originallink') else '',
                    'date': item.get('pubDate', ''),
                    'collection': source_type
                }
                full_text = f"ë‰´ìŠ¤ ì œëª©: {title}\në‰´ìŠ¤ ë‚´ìš©: {content}\nì–¸ë¡ ì‚¬: {metadata.get('publisher', '')}"
            elif source_type == "ë¸”ë¡œê·¸":
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'bloggername': item.get('bloggername', ''),
                    'date': item.get('postdate', ''),
                    'collection': source_type
                }
                full_text = f"ì œëª©: {title}\në‚´ìš©: {content}\në¸”ë¡œê±°: {metadata.get('bloggername', '')}"
            else:  # ì‡¼í•‘
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'lprice': item.get('lprice', ''),
                    'mallname': item.get('mallName', ''),
                    'brand': item.get('brand', ''),
                    'collection': source_type
                }
                full_text = f"ìƒí’ˆëª…: {title}\nì„¤ëª…: {content}\në¸Œëœë“œ: {metadata.get('brand', '')}"
            
            # ì„ë² ë”© ìƒì„±
            embedding = generate_embedding(full_text)
            if embedding:
                # ì¤‘ë³µ ì²´í¬
                existing = supabase.table('documents').select('id').eq('metadata->>url', metadata.get('url', '')).execute()
                
                if not existing.data:
                    data = {
                        'content': full_text,
                        'embedding': embedding,
                        'metadata': metadata
                    }
                    supabase.table('documents').insert(data).execute()
                    saved_count += 1
                    
        except Exception as e:
            continue
    
    return saved_count

def semantic_search_sync(query: str, source_type: str) -> List[Dict]:
    """ì‹œë§¨í‹± ê²€ìƒ‰ (ë™ê¸°)"""
    try:
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        if source_type == "ë‰´ìŠ¤":
            processed_query = f"ë‰´ìŠ¤ ê²€ìƒ‰: {query} ë‰´ìŠ¤ ê¸°ì‚¬ ì–¸ë¡ ì‚¬ ë³´ë„"
        elif source_type == "ì‡¼í•‘":
            processed_query = f"ìƒí’ˆ ê²€ìƒ‰: {query} ì‡¼í•‘ ìƒí’ˆ ê°€ê²©"
        else:
            processed_query = f"ë¸”ë¡œê·¸ ê²€ìƒ‰: {query} ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…"
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = generate_embedding(processed_query)
        
        if query_embedding is None:
            return []
        
        # ë²¡í„° ê²€ìƒ‰
        response = supabase.rpc(
            'match_documents',
            {
                'query_embedding': query_embedding,
                'match_threshold': 0.3,
                'match_count': 50
            }
        ).execute()
        
        # ì†ŒìŠ¤ íƒ€ì… í•„í„°ë§
        filtered_results = []
        if response.data:
            for item in response.data:
                metadata = item.get('metadata', {})
                if isinstance(metadata, str):
                    metadata = json.loads(metadata)
                
                if metadata.get('collection') == source_type:
                    filtered_results.append(item)
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        filtered_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        return filtered_results[:10]
        
    except Exception as e:
        st.error(f"ì‹œë§¨í‹± ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def generate_answer_with_gpt(query: str, results: List[Dict], source_type: str) -> str:
    """GPTë¡œ ë‹µë³€ ìƒì„±"""
    if not results:
        return f"'{query}'ì— ëŒ€í•œ {source_type} ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    contexts = []
    for i, result in enumerate(results[:5]):
        content = result['content']
        metadata = result.get('metadata', {})
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        
        title = metadata.get('title', 'ì œëª© ì—†ìŒ')
        similarity = result.get('similarity', 0) * 100
        
        contexts.append(f"ë¬¸ì„œ {i+1} - {title} (ìœ ì‚¬ë„: {similarity:.1f}%):\n{content}\n")
    
    context_text = "\n".join(contexts)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompts = {
        "ë¸”ë¡œê·¸": """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¸”ë¡œê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , êµ¬ì²´ì ì¸ íŒì´ë‚˜ ê²½í—˜ë‹´ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.""",
        
        "ë‰´ìŠ¤": """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ì‹¤ì ì´ê³  ê· í˜•ì¡íŒ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ìµœì‹  ë™í–¥ê³¼ ë‹¤ì–‘í•œ ê´€ì ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.""",
        
        "ì‡¼í•‘": """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ì‡¼í•‘ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ìƒí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì‡¼í•‘ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ê°€ê²© ë¹„êµë‚˜ ì œí’ˆ íŠ¹ì§•ì„ í¬í•¨í•´ ì£¼ì„¸ìš”."""
    }
    
    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
    user_prompts = {
        "ë¸”ë¡œê·¸": f"""ë‹¤ìŒì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤:

{context_text}

ì§ˆë¬¸: {query}

ìœ„ ë¸”ë¡œê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ìœ ìš©í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì‹¤ì œ ê²½í—˜ë‹´ì´ë‚˜ êµ¬ì²´ì ì¸ íŒì´ ìˆë‹¤ë©´ í¬í•¨í•´ì£¼ì„¸ìš”.""",
        
        "ë‰´ìŠ¤": f"""ë‹¤ìŒì€ ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤:

{context_text}

ì§ˆë¬¸: {query}

ìœ„ ë‰´ìŠ¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê°ê´€ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ìµœì‹  ë™í–¥ì´ë‚˜ ì „ë¬¸ê°€ ì˜ê²¬ì´ ìˆë‹¤ë©´ í¬í•¨í•´ì£¼ì„¸ìš”.""",
        
        "ì‡¼í•‘": f"""ë‹¤ìŒì€ ë„¤ì´ë²„ ì‡¼í•‘ì—ì„œ ìˆ˜ì§‘í•œ ìƒí’ˆ ì •ë³´ì…ë‹ˆë‹¤:

{context_text}

ì§ˆë¬¸: {query}

ìœ„ ìƒí’ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì‹¤ìš©ì ì¸ ì‡¼í•‘ ì¡°ì–¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ê°€ê²©ëŒ€ë‚˜ ì œí’ˆ íŠ¹ì§• ë¹„êµê°€ ìˆë‹¤ë©´ í¬í•¨í•´ì£¼ì„¸ìš”."""
    }
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompts[source_type]},
                {"role": "user", "content": user_prompts[source_type]}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        return response.choices[0].message.content
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ========== Streamlit UI ==========
st.title("ğŸ›ï¸ ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ íŒŒì¸ë”: LangGraph Enhanced")
st.write("AI ì—ì´ì „íŠ¸ê°€ ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ”§ ì„¤ì •")
    
    search_mode = st.radio(
        "ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ",
        options=["ì‹œë§¨í‹± ê²€ìƒ‰ (ì €ì¥ëœ ë°ì´í„°)", "ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"],
        index=0,
        help="ì €ì¥ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
    )
    
    st.markdown("---")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
    st.markdown("### ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ")
    try:
        result = supabase.table('documents').select('id', count='exact').execute()
        doc_count = result.count if hasattr(result, 'count') else len(result.data)
        st.metric("ì´ ë¬¸ì„œ ìˆ˜", f"{doc_count:,}ê°œ")
    except Exception as e:
        st.error("DB ì—°ê²° ì˜¤ë¥˜")

# ë©”ì¸ ì½˜í…ì¸ 
col1, col2 = st.columns([3, 1])

with col1:
    # ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ
    source_options = ["ì‡¼í•‘", "ë¸”ë¡œê·¸", "ë‰´ìŠ¤"]
    selected_source = st.radio(
        "ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ",
        options=source_options,
        horizontal=True
    )
    
    # ì§ˆë¬¸ ì…ë ¥
    query = st.text_input(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
        placeholder=f"{selected_source} ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ìµœì‹  ìŠ¤ë§ˆíŠ¸í° ì¶”ì²œ)",
        help=f"{selected_source}ì—ì„œ ê²€ìƒ‰í•  ë‚´ìš©ì„ ìì„¸íˆ ì…ë ¥í•´ì£¼ì„¸ìš”."
    )

with col2:
    st.markdown("### ğŸ¯ ê²€ìƒ‰ íŒ")
    st.markdown("""
    - êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš©
    - ë¸Œëœë“œëª…ì´ë‚˜ ëª¨ë¸ëª… í¬í•¨
    - ê°€ê²©ëŒ€ë‚˜ ì¡°ê±´ ëª…ì‹œ
    """)

# ê²€ìƒ‰ ì‹¤í–‰
if st.button(f"ğŸ” {selected_source}ì—ì„œ ê²€ìƒ‰í•˜ê¸°", type="primary", use_container_width=True):
    if query.strip():
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # ë‹¨ê³„ 1: ë„¤ì´ë²„ ê²€ìƒ‰
            status_text.info("ğŸ” ë„¤ì´ë²„ì—ì„œ ê²€ìƒ‰ ì¤‘...")
            progress_bar.progress(20)
            
            search_result = search_naver_sync(query, selected_source)
            
            if not search_result["success"]:
                st.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {search_result['error']}")
                st.stop()
            
            naver_results = search_result["items"]
            st.success(f"âœ… {len(naver_results)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            if search_mode == "ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥":
                # ë‹¨ê³„ 2: ì „ì²´ ê¸°ì‚¬ ìˆ˜ì§‘ (ë‰´ìŠ¤ì˜ ê²½ìš°)
                if selected_source == "ë‰´ìŠ¤":
                    status_text.info("ğŸ“° ì „ì²´ ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ì¤‘...")
                    progress_bar.progress(40)
                    
                    full_articles = fetch_full_articles_sync(naver_results, selected_source)
                    full_count = sum(1 for a in full_articles if 'full_content' in a)
                    if full_count > 0:
                        st.success(f"âœ… {full_count}ê°œì˜ ì „ì²´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                else:
                    full_articles = naver_results
                
                # ë‹¨ê³„ 3: Supabaseì— ì €ì¥
                status_text.info("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
                progress_bar.progress(60)
                
                saved_count = save_to_supabase_sync(full_articles, selected_source)
                st.success(f"âœ… {saved_count}ê°œì˜ ë¬¸ì„œë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                
                progress_bar.progress(80)
            
            # ë‹¨ê³„ 4: ì‹œë§¨í‹± ê²€ìƒ‰
            status_text.info("ğŸ§  AI ë¶„ì„ ì¤‘...")
            progress_bar.progress(90)
            
            semantic_results = semantic_search_sync(query, selected_source)
            
            if not semantic_results:
                st.warning("ì €ì¥ëœ ë°ì´í„°ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥' ëª¨ë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                st.stop()
            
            # ë‹¨ê³„ 5: AI ë‹µë³€ ìƒì„±
            status_text.info("âœ¨ AI ë‹µë³€ ìƒì„± ì¤‘...")
            
            final_answer = generate_answer_with_gpt(query, semantic_results, selected_source)
            
            progress_bar.progress(100)
            status_text.success("âœ… ê²€ìƒ‰ ì™„ë£Œ!")
            
            # ê²°ê³¼ í‘œì‹œ
            st.markdown("---")
            st.markdown("## ğŸ¤– AI ë‹µë³€")
            st.markdown(final_answer)
            
            # ìƒì„¸ ê²°ê³¼ í‘œì‹œ
            with st.expander("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë³´ê¸°", expanded=False):
                st.markdown(f"### {len(semantic_results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤")
                
                for i, item in enumerate(semantic_results):
                    metadata = item.get('metadata', {})
                    if isinstance(metadata, str):
                        metadata = json.loads(metadata)
                    
                    similarity = item.get('similarity', 0) * 100
                    title = metadata.get('title', 'ì œëª© ì—†ìŒ')
                    
                    with st.container():
                        st.markdown(f"**{i+1}. {title}** (ìœ ì‚¬ë„: {similarity:.1f}%)")
                        
                        # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                        col1, col2 = st.columns(2)
                        with col1:
                            if metadata.get('url'):
                                st.markdown(f"ğŸ”— [ì›ë³¸ ë³´ê¸°]({metadata['url']})")
                        with col2:
                            if selected_source == "ë‰´ìŠ¤" and metadata.get('publisher'):
                                st.markdown(f"ğŸ“° {metadata['publisher']}")
                            elif selected_source == "ë¸”ë¡œê·¸" and metadata.get('bloggername'):
                                st.markdown(f"âœï¸ {metadata['bloggername']}")
                            elif selected_source == "ì‡¼í•‘" and metadata.get('brand'):
                                st.markdown(f"ğŸ·ï¸ {metadata['brand']}")
                        
                        # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                        content_preview = item['content'][:200] + "..." if len(item['content']) > 200 else item['content']
                        st.markdown(f"> {content_preview}")
                        st.markdown("---")
            
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        finally:
            # ì§„í–‰ ìƒí™© í‘œì‹œ ì œê±°
            progress_bar.empty()
            status_text.empty()
    else:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
with st.expander("ğŸš€ LangGraph ê°œì„ ì‚¬í•­", expanded=False):
    st.markdown("""
    **ìƒˆë¡œìš´ ê¸°ëŠ¥:**
    - ğŸ”„ **ì§€ëŠ¥ì  ì›Œí¬í”Œë¡œìš°**: ì½˜í…ì¸  í’ˆì§ˆì— ë”°ë¼ ìë™ìœ¼ë¡œ ì „ì²´ ê¸°ì‚¬ ìˆ˜ì§‘
    - âš¡ **ë³‘ë ¬ ì²˜ë¦¬**: ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ë™ì‹œì— ê°€ì ¸ì™€ ì†ë„ í–¥ìƒ
    - ğŸ“Š **í’ˆì§ˆ í‰ê°€**: ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€
    - ğŸ§  **ìƒíƒœ ê´€ë¦¬**: ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ì¶”ì 
    - ğŸ” **ìë™ ì¬ì‹œë„**: ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ëŒ€ì²´ ì „ëµ ì‚¬ìš©
    - ğŸ’¾ **ìºì‹± ìµœì í™”**: ì„ë² ë”© ëª¨ë¸ê³¼ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
    
    **ê°œì„ ëœ ì‚¬ìš©ì ê²½í—˜:**
    - ğŸ¯ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
    - ğŸ“± ë°˜ì‘í˜• UI ë””ìì¸
    - ğŸ¨ ì§ê´€ì ì¸ ê²°ê³¼ í‘œì‹œ
    - ğŸ” ìƒì„¸í•œ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
    """)

