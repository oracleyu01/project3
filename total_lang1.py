import streamlit as st
import os
import json
import numpy as np
import urllib.request
import urllib.parse
import re
import pandas as pd
from datetime import datetime
import asyncio
import aiohttp
from typing import TypedDict, List, Dict, Optional, Literal
from bs4 import BeautifulSoup
import time

from supabase import create_client
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from langgraph.graph import StateGraph, END
from langgraph.checkpoint import MemorySaver

# LangSmith ì¶”ì  ì„¤ì •
from langsmith import Client
from langchain_core.tracers.context import tracing_v2_enabled

# í˜ì´ì§€ êµ¬ì„±
st.set_page_config(page_title="ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ íŒŒì¸ë” (LangGraph Enhanced)", layout="wide")

# ========== API í‚¤ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ==========
try:
    # Streamlit Cloud í™˜ê²½ì—ì„œëŠ” st.secrets ì‚¬ìš©
    supabase_url = st.secrets["SUPABASE_URL"]
    supabase_key = st.secrets["SUPABASE_KEY"]
    openai_api_key = st.secrets["OPENAI_API_KEY"]
    NAVER_CLIENT_ID = st.secrets["NAVER_CLIENT_ID"]
    NAVER_CLIENT_SECRET = st.secrets["NAVER_CLIENT_SECRET"]
    
    # LangSmith API í‚¤ ì¶”ê°€
    LANGCHAIN_API_KEY = st.secrets["LANGCHAIN_API_KEY"]
    LANGCHAIN_PROJECT = st.secrets.get("LANGCHAIN_PROJECT", "smart-shopping-finder")
    
except Exception as e:
    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
    try:
        import dotenv
        dotenv.load_dotenv()
        supabase_url = os.environ.get("SUPABASE_URL")
        supabase_key = os.environ.get("SUPABASE_KEY")
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
        NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
        
        # LangSmith API í‚¤ ì¶”ê°€
        LANGCHAIN_API_KEY = os.environ.get("LANGCHAIN_API_KEY")
        LANGCHAIN_PROJECT = os.environ.get("LANGCHAIN_PROJECT", "smart-shopping-finder")

    except:
        st.error("API í‚¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë‚˜ Streamlit Secretsê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

# API í‚¤ í™•ì¸
if not supabase_url or not supabase_key or not openai_api_key:
    st.error("í•„ìš”í•œ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# LangSmith ì„¤ì • (ì„ íƒì )
if LANGCHAIN_API_KEY:
    # LangSmith í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
    os.environ["LANGCHAIN_PROJECT"] = LANGCHAIN_PROJECT
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
    
    # LangSmith í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langsmith_client = Client(api_key=LANGCHAIN_API_KEY)
    
    st.sidebar.success(f"âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨ (í”„ë¡œì íŠ¸: {LANGCHAIN_PROJECT})")
    
    # ì¶”ì  URL í‘œì‹œ
    st.sidebar.info(
        f"ì¶”ì  ëŒ€ì‹œë³´ë“œ: [LangSmith]"
        f"(https://smith.langchain.com/o/YOUR_ORG/projects/p/{LANGCHAIN_PROJECT})"
    )
else:
    st.sidebar.warning("âš ï¸ LangSmith API í‚¤ê°€ ì—†ì–´ ì¶”ì ì´ ë¹„í™œì„±í™”ë¨")
    os.environ["LANGCHAIN_TRACING_V2"] = "false"

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    supabase = create_client(supabase_url, supabase_key)
    st.sidebar.success("âœ… Supabase ì—°ê²° ì„±ê³µ!")
except Exception as e:
    st.error(f"Supabase ì—°ê²° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.stop()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    openai_client = OpenAI(api_key=openai_api_key)
    st.sidebar.success("âœ… OpenAI ì—°ê²° ì„±ê³µ!")
except Exception as e:
    st.error(f"OpenAI ì—°ê²° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.stop()

# ë‚˜ë¨¸ì§€ ì´ˆê¸°í™” ì½”ë“œ...
# ========== ìƒíƒœ ì •ì˜ ==========
class SearchState(TypedDict):
    query: str
    source_type: str
    search_mode: str
    naver_results: List[Dict]
    full_articles: List[Dict]
    embeddings: List[List[float]]
    semantic_results: List[Dict]
    final_answer: str
    error: Optional[str]
    retry_count: int
    quality_score: float
    status_message: str

# ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ==========
@st.cache_resource
def load_embedding_model():
    """í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
    try:
        model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        return model
    except:
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        return model

embedding_model = load_embedding_model()

def generate_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    try:
        if not text or len(text.strip()) < 10:
            return None
        
        cleaned_text = re.sub(r'\s+', ' ', text.strip())
        cleaned_text = re.sub(r'[^\w\sê°€-í£\.]', ' ', cleaned_text)
        
        if len(cleaned_text) > 512:
            cleaned_text = cleaned_text[:512]
        
        embedding = embedding_model.encode(cleaned_text, convert_to_tensor=False)
        embedding_list = embedding.tolist() if hasattr(embedding, 'tolist') else embedding
        
        # 768ì°¨ì›ì„ 1536ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
        if len(embedding_list) == 768:
            return embedding_list + [0.0] * (1536 - 768)
        elif len(embedding_list) == 1536:
            return embedding_list
        else:
            if len(embedding_list) < 1536:
                return embedding_list + [0.0] * (1536 - len(embedding_list))
            else:
                return embedding_list[:1536]
    except Exception as e:
        return None

# ========== ë‰´ìŠ¤ ì „ì²´ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ==========
async def fetch_full_article(session: aiohttp.ClientSession, url: str) -> Optional[str]:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ì „ì²´ ë‚´ìš© ë¹„ë™ê¸°ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        async with session.get(url, headers=headers, timeout=10) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('article', {'id': 'dic_area'})
            if article_body:
                return article_body.get_text(strip=True)
            
            # ë‹¤ë¥¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì„ íƒìë“¤
            selectors = [
                'div.article_body', 'div.news_view', 'div.articleBody',
                'div.content', 'div.article-body', 'div#articleBodyContents'
            ]
            for selector in selectors:
                content = soup.select_one(selector)
                if content:
                    return content.get_text(strip=True)
            
            return None
    except Exception as e:
        return None

# ========== LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤ ==========
async def search_naver_node(state: SearchState) -> Dict:
    """ë„¤ì´ë²„ API ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        query = state["query"]
        source_type = state["source_type"]
        
        # API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
        api_endpoint = {
            "ë¸”ë¡œê·¸": "blog",
            "ë‰´ìŠ¤": "news", 
            "ì‡¼í•‘": "shop"
        }.get(source_type, "blog")
        
        encoded_query = urllib.parse.quote(query)
        url = f"https://openapi.naver.com/v1/search/{api_endpoint}?query={encoded_query}&display=20&sort=sim"
        
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
        request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
        
        response = urllib.request.urlopen(request, timeout=15)
        if response.getcode() == 200:
            response_data = json.loads(response.read().decode('utf-8'))
            items = response_data.get('items', [])
            
            return {
                "naver_results": items,
                "status_message": f"{len(items)}ê°œ ê²€ìƒ‰ ê²°ê³¼ ë°œê²¬"
            }
        else:
            return {"error": "ë„¤ì´ë²„ API ì˜¤ë¥˜", "naver_results": []}
            
    except Exception as e:
        return {"error": str(e), "naver_results": []}

async def check_content_quality_node(state: SearchState) -> Dict:
    """ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ ë…¸ë“œ"""
    results = state.get("naver_results", [])
    source_type = state["source_type"]
    
    if not results:
        return {"quality_score": 0.0}
    
    total_content_length = 0
    for item in results:
        if source_type == "ë‰´ìŠ¤":
            content = item.get('description', '')
        else:
            content = item.get('description', '') + item.get('title', '')
        total_content_length += len(re.sub('<[^<]+?>', '', content))
    
    avg_length = total_content_length / len(results)
    
    # ë‰´ìŠ¤ëŠ” ë” ë‚®ì€ ê¸°ì¤€ ì ìš©
    if source_type == "ë‰´ìŠ¤":
        quality_score = min(avg_length / 100, 1.0)  # 100ì ì´ìƒì´ë©´ ë†’ì€ í’ˆì§ˆ
    else:
        quality_score = min(avg_length / 200, 1.0)  # 200ì ì´ìƒì´ë©´ ë†’ì€ í’ˆì§ˆ
    
    return {
        "quality_score": quality_score,
        "status_message": f"ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}"
    }

async def fetch_full_articles_node(state: SearchState) -> Dict:
    """ì „ì²´ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ë…¸ë“œ (ë³‘ë ¬ ì²˜ë¦¬)"""
    if state["source_type"] != "ë‰´ìŠ¤":
        return {"full_articles": state["naver_results"]}
    
    results = state["naver_results"]
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for item in results[:10]:  # ìƒìœ„ 10ê°œë§Œ ì²˜ë¦¬
            url = item.get('link', '')
            if url:
                tasks.append(fetch_full_article(session, url))
        
        full_contents = await asyncio.gather(*tasks)
    
    # ì „ì²´ ë‚´ìš©ì„ ê²°ê³¼ì— ì¶”ê°€
    full_articles = []
    for i, item in enumerate(results[:10]):
        full_content = full_contents[i] if i < len(full_contents) else None
        
        if full_content:
            item_copy = item.copy()
            item_copy['full_content'] = full_content
            full_articles.append(item_copy)
        else:
            full_articles.append(item)
    
    # ë‚˜ë¨¸ì§€ í•­ëª©ë“¤ë„ ì¶”ê°€
    full_articles.extend(results[10:])
    
    successful_fetches = sum(1 for c in full_contents if c)
    return {
        "full_articles": full_articles,
        "status_message": f"{successful_fetches}ê°œ ì „ì²´ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ"
    }

async def save_to_supabase_node(state: SearchState) -> Dict:
    """Supabaseì— ì €ì¥í•˜ëŠ” ë…¸ë“œ"""
    articles = state.get("full_articles", state.get("naver_results", []))
    source_type = state["source_type"]
    saved_count = 0
    
    for item in articles:
        try:
            # HTML íƒœê·¸ ì œê±°
            title = re.sub('<[^<]+?>', '', item.get('title', ''))
            
            # ì „ì²´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ description ì‚¬ìš©
            if 'full_content' in item and item['full_content']:
                content = item['full_content']
            else:
                content = re.sub('<[^<]+?>', '', item.get('description', ''))
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            if source_type == "ë‰´ìŠ¤":
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'publisher': item.get('originallink', '').replace('https://', '').replace('http://', '').split('/')[0] if item.get('originallink') else '',
                    'date': item.get('pubDate', ''),
                    'collection': source_type
                }
                full_text = f"ë‰´ìŠ¤ ì œëª©: {title}\në‰´ìŠ¤ ë‚´ìš©: {content}\nì–¸ë¡ ì‚¬: {metadata.get('publisher', '')}"
            elif source_type == "ë¸”ë¡œê·¸":
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'bloggername': item.get('bloggername', ''),
                    'date': item.get('postdate', ''),
                    'collection': source_type
                }
                full_text = f"ì œëª©: {title}\në‚´ìš©: {content}\në¸”ë¡œê±°: {metadata.get('bloggername', '')}"
            else:  # ì‡¼í•‘
                metadata = {
                    'title': title,
                    'url': item.get('link', ''),
                    'lprice': item.get('lprice', ''),
                    'mallname': item.get('mallName', ''),
                    'brand': item.get('brand', ''),
                    'collection': source_type
                }
                full_text = f"ìƒí’ˆëª…: {title}\nì„¤ëª…: {content}\në¸Œëœë“œ: {metadata.get('brand', '')}"
            
            # ì„ë² ë”© ìƒì„±
            embedding = generate_embedding(full_text)
            if embedding:
                # ì¤‘ë³µ ì²´í¬
                existing = supabase.table('documents').select('id').eq(f"metadata->>url", metadata.get('url', '')).execute()
                
                if not existing.data:
                    data = {
                        'content': full_text,
                        'embedding': embedding,
                        'metadata': metadata
                    }
                    supabase.table('documents').insert(data).execute()
                    saved_count += 1
                    
        except Exception as e:
            continue
    
    return {
        "status_message": f"{saved_count}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ"
    }

async def semantic_search_node(state: SearchState) -> Dict:
    """ì‹œë§¨í‹± ê²€ìƒ‰ ë…¸ë“œ"""
    try:
        query = state["query"]
        source_type = state["source_type"]
        
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        if source_type == "ë‰´ìŠ¤":
            processed_query = f"ë‰´ìŠ¤ ê²€ìƒ‰: {query} ë‰´ìŠ¤ ê¸°ì‚¬ ì–¸ë¡ ì‚¬ ë³´ë„"
        elif source_type == "ì‡¼í•‘":
            processed_query = f"ìƒí’ˆ ê²€ìƒ‰: {query} ì‡¼í•‘ ìƒí’ˆ ê°€ê²©"
        else:
            processed_query = f"ë¸”ë¡œê·¸ ê²€ìƒ‰: {query} ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…"
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = generate_embedding(processed_query)
        
        if query_embedding is None:
            return {"semantic_results": [], "error": "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"}
        
        # ë²¡í„° ê²€ìƒ‰
        response = supabase.rpc(
            'match_documents',
            {
                'query_embedding': query_embedding,
                'match_threshold': 0.3,
                'match_count': 50
            }
        ).execute()
        
        # ì†ŒìŠ¤ íƒ€ì… í•„í„°ë§
        filtered_results = []
        if response.data:
            for item in response.data:
                metadata = item.get('metadata', {})
                if isinstance(metadata, str):
                    metadata = json.loads(metadata)
                
                if metadata.get('collection') == source_type:
                    filtered_results.append(item)
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        filtered_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        return {
            "semantic_results": filtered_results[:10],
            "status_message": f"{len(filtered_results)}ê°œ ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼"
        }
        
    except Exception as e:
        return {"semantic_results": [], "error": str(e)}

async def generate_answer_node(state: SearchState) -> Dict:
    """GPT ë‹µë³€ ìƒì„± ë…¸ë“œ"""
    results = state.get("semantic_results", [])
    query = state["query"]
    source_type = state["source_type"]
    
    if not results:
        return {
            "final_answer": f"'{query}'ì— ëŒ€í•œ {source_type} ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "status_message": "ë‹µë³€ ìƒì„± ì™„ë£Œ"
        }
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ê¸°ì¡´ generate_answer_with_gpt ë¡œì§ ì‚¬ìš©)
    contexts = []
    for i, result in enumerate(results[:5]):
        content = result['content']
        metadata = result.get('metadata', {})
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        
        title = metadata.get('title', 'ì œëª© ì—†ìŒ')
        similarity = result.get('similarity', 0) * 100
        
        contexts.append(f"ë¬¸ì„œ {i+1} - {title} (ìœ ì‚¬ë„: {similarity:.1f}%):\n{content}\n")
    
    context_text = "\n".join(contexts)
    
    # GPT í˜¸ì¶œ
    system_prompt = get_system_prompt(source_type)
    user_prompt = get_user_prompt(query, context_text, source_type)
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        return {
            "final_answer": response.choices[0].message.content,
            "status_message": "ë‹µë³€ ìƒì„± ì™„ë£Œ"
        }
    except Exception as e:
        return {
            "final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "error": str(e)
        }

# ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ë“¤ ì¬ì‚¬ìš©
def get_system_prompt(source_type):
    # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼
    if source_type == "ë¸”ë¡œê·¸":
        return """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤..."""
    elif source_type == "ë‰´ìŠ¤":
        return """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤..."""
    elif source_type == "ì‡¼í•‘":
        return """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ì‡¼í•‘ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤..."""

def get_user_prompt(query, context_text, source_type):
    # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼
    if source_type == "ë¸”ë¡œê·¸":
        return f"""ë‹¤ìŒì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤..."""
    elif source_type == "ë‰´ìŠ¤":
        return f"""ë‹¤ìŒì€ ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤..."""
    elif source_type == "ì‡¼í•‘":
        return f"""ë‹¤ìŒì€ ë„¤ì´ë²„ ì‡¼í•‘ì—ì„œ ìˆ˜ì§‘í•œ ìƒí’ˆ ì •ë³´ì…ë‹ˆë‹¤..."""

# ========== LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ==========
class SmartSearchSystem:
    def __init__(self):
        self.memory = MemorySaver()
        self.workflow = self.build_workflow()
        self.app = self.workflow.compile(checkpointer=self.memory)
    
    def build_workflow(self):
        workflow = StateGraph(SearchState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("search_naver", search_naver_node)
        workflow.add_node("check_quality", check_content_quality_node)
        workflow.add_node("fetch_full_articles", fetch_full_articles_node)
        workflow.add_node("save_to_supabase", save_to_supabase_node)
        workflow.add_node("semantic_search", semantic_search_node)
        workflow.add_node("generate_answer", generate_answer_node)
        
        # ì‹œì‘ì  ì„¤ì •
        workflow.set_entry_point("search_naver")
        
        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge("search_naver", "check_quality")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ - í’ˆì§ˆì— ë”°ë¼ ë¶„ê¸°
        workflow.add_conditional_edges(
            "check_quality",
            self.quality_router,
            {
                "fetch_full": "fetch_full_articles",
                "skip_fetch": "save_to_supabase"
            }
        )
        
        workflow.add_edge("fetch_full_articles", "save_to_supabase")
        
        # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ë¶„ê¸°
        workflow.add_conditional_edges(
            "save_to_supabase",
            self.mode_router,
            {
                "semantic": "semantic_search",
                "end": END
            }
        )
        
        workflow.add_edge("semantic_search", "generate_answer")
        workflow.add_edge("generate_answer", END)
        
        return workflow
    
    def quality_router(self, state: SearchState):
        """í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¼ ì „ì²´ ê¸°ì‚¬ ê°€ì ¸ì˜¬ì§€ ê²°ì •"""
        quality_score = state.get("quality_score", 0)
        source_type = state["source_type"]
        
        # ë‰´ìŠ¤ì´ê³  í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ì „ì²´ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        if source_type == "ë‰´ìŠ¤" and quality_score < 0.5:
            return "fetch_full"
        
        return "skip_fetch"
    
    def mode_router(self, state: SearchState):
        """ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
        if state["search_mode"] == "ì‹œë§¨í‹± ê²€ìƒ‰ (ì €ì¥ëœ ë°ì´í„°)":
            return "semantic"
        return "end"
    
    async def search(self, query: str, source_type: str, search_mode: str):
        """í†µí•© ê²€ìƒ‰ ì‹¤í–‰"""
        initial_state = {
            "query": query,
            "source_type": source_type,
            "search_mode": search_mode,
            "retry_count": 0,
            "status_message": "ê²€ìƒ‰ ì‹œì‘..."
        }
        
        config = {"configurable": {"thread_id": f"{query}-{source_type}"}}
        
        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ë°›ê¸°
        status_placeholder = st.empty()
        
        async for event in self.app.astream_events(initial_state, config=config, version="v1"):
            if event["event"] == "on_node_end":
                node_output = event["data"]["output"]
                if "status_message" in node_output:
                    status_placeholder.info(node_output["status_message"])
        
        # ìµœì¢… ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        final_state = await self.app.aget_state(config)
        return final_state.values

# ========== Streamlit UI ==========
st.title("ğŸ›ï¸ ìŠ¤ë§ˆíŠ¸ ì‡¼í•‘ íŒŒì¸ë”: LangGraph Enhanced")
st.write("AI ì—ì´ì „íŠ¸ê°€ ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "search_system" not in st.session_state:
    st.session_state.search_system = SmartSearchSystem()

# UI êµ¬ì„± (ê¸°ì¡´ê³¼ ìœ ì‚¬)
search_mode = st.sidebar.radio(
    "ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ",
    options=["ì‹œë§¨í‹± ê²€ìƒ‰ (ì €ì¥ëœ ë°ì´í„°)", "ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥"],
    index=0
)

source_options = ["ì‡¼í•‘", "ë¸”ë¡œê·¸", "ë‰´ìŠ¤"]
selected_source = st.radio(
    "ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ",
    options=source_options,
    horizontal=True
)

query = st.text_input(
    "ì§ˆë¬¸ ì…ë ¥",
    placeholder=f"{selected_source} ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"
)

# ê²€ìƒ‰ ì‹¤í–‰
if st.button(f"{selected_source}ì—ì„œ {search_mode.split()[0]}", type="primary"):
    if query:
        with st.spinner("ì§€ëŠ¥ì  ê²€ìƒ‰ ì§„í–‰ ì¤‘..."):
            # ë¹„ë™ê¸° ê²€ìƒ‰ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            result = loop.run_until_complete(
                st.session_state.search_system.search(query, selected_source, search_mode)
            )
            
            # ê²°ê³¼ í‘œì‹œ
            if result.get("final_answer"):
                st.markdown("## ğŸ¤– AI ë‹µë³€")
                st.markdown(result["final_answer"])
                
                # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ í‘œì‹œ
                if st.checkbox("ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë³´ê¸°"):
                    semantic_results = result.get("semantic_results", [])
                    if semantic_results:
                        st.markdown("### ğŸ“Š ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼")
                        for i, item in enumerate(semantic_results):
                            metadata = item.get('metadata', {})
                            if isinstance(metadata, str):
                                metadata = json.loads(metadata)
                            
                            similarity = item.get('similarity', 0) * 100
                            with st.expander(f"{i+1}. {metadata.get('title', 'ì œëª© ì—†ìŒ')} (ìœ ì‚¬ë„: {similarity:.1f}%)"):
                                st.write(item['content'])
                                if metadata.get('url'):
                                    st.markdown(f"[ì›ë³¸ ë³´ê¸°]({metadata['url']})")
            
            # í”„ë¡œì„¸ìŠ¤ í†µê³„
            with st.sidebar:
                st.markdown("### ğŸ” ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤")
                if result.get("naver_results"):
                    st.success(f"âœ… ë„¤ì´ë²„ ê²€ìƒ‰: {len(result['naver_results'])}ê°œ")
                if result.get("full_articles"):
                    full_count = sum(1 for a in result['full_articles'] if 'full_content' in a)
                    if full_count > 0:
                        st.success(f"âœ… ì „ì²´ ê¸°ì‚¬ ìˆ˜ì§‘: {full_count}ê°œ")
                if result.get("semantic_results"):
                    st.success(f"âœ… ì‹œë§¨í‹± ë§¤ì¹­: {len(result['semantic_results'])}ê°œ")
                if result.get("quality_score") is not None:
                    st.info(f"ğŸ“Š ì½˜í…ì¸  í’ˆì§ˆ: {result['quality_score']:.2f}")
    else:
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")

# ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ (ê¸°ì¡´ê³¼ ë™ì¼)
st.sidebar.title("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ")
try:
    result = supabase.table('documents').select('id', count='exact').execute()
    doc_count = result.count if hasattr(result, 'count') else len(result.data)
    st.sidebar.metric("ì´ ë¬¸ì„œ ìˆ˜", f"{doc_count:,}ê°œ")
except:
    st.sidebar.error("DB ì—°ê²° ì˜¤ë¥˜")

# LangGraph íŠ¹ì§• ì•ˆë‚´
with st.sidebar.expander("ğŸš€ LangGraph ê°œì„ ì‚¬í•­"):
    st.markdown("""
    **ìƒˆë¡œìš´ ê¸°ëŠ¥:**
    - ğŸ”„ ì§€ëŠ¥ì  ì›Œí¬í”Œë¡œìš°: ì½˜í…ì¸  í’ˆì§ˆì— ë”°ë¼ ìë™ìœ¼ë¡œ ì „ì²´ ê¸°ì‚¬ ìˆ˜ì§‘
    - âš¡ ë³‘ë ¬ ì²˜ë¦¬: ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ë™ì‹œì— ê°€ì ¸ì™€ ì†ë„ í–¥ìƒ
    - ğŸ“Š í’ˆì§ˆ í‰ê°€: ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ í‰ê°€
    - ğŸ§  ìƒíƒœ ê´€ë¦¬: ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ì¶”ì 
    - ğŸ” ìë™ ì¬ì‹œë„: ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ëŒ€ì²´ ì „ëµ ì‚¬ìš©
    """)
